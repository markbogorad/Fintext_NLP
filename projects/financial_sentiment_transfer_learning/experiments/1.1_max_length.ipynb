{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert a comprehensive markdown table of experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports (edit later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 512 Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Train-test-validation split\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Convert to TensorFlow dataset\n",
    "def to_tf_dataset(split, shuffle=False):\n",
    "    return split.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=[\"label\"],\n",
    "        shuffle=shuffle,\n",
    "        batch_size=8,\n",
    "        collate_fn=None\n",
    "    )\n",
    "\n",
    "tf_train_dataset = to_tf_dataset(train_val_split['train'], shuffle=True)\n",
    "tf_validation_dataset = to_tf_dataset(val_test_split['train'], shuffle=True)\n",
    "tf_test_dataset = to_tf_dataset(val_test_split['test'], shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Freeze DistilBERT encoder\n",
    "model.distilbert.trainable = False\n",
    "\n",
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {eval_loss:.4f}, Test Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix & Classification Report\n",
    "y_pred_logits = model.predict(tf_test_dataset).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "\n",
    "y_true = np.concatenate([y for x, y in tf_test_dataset], axis=0)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Experiment 1B: Reduce Max Length Only (no class weights)\n",
    "\n",
    "# Tokenization (new max_length=128)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Train-test-validation split (same)\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Convert to TensorFlow datasets (same)\n",
    "tf_train_dataset = to_tf_dataset(train_val_split['train'], shuffle=True)\n",
    "tf_validation_dataset = to_tf_dataset(val_test_split['train'], shuffle=True)\n",
    "tf_test_dataset = to_tf_dataset(val_test_split['test'], shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Freeze encoder\n",
    "model.distilbert.trainable = False\n",
    "\n",
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train (no class weights)\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {eval_loss:.4f}, Test Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy (Reduced Max Length)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (Reduced Max Length)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix & Classification Report\n",
    "y_pred_logits = model.predict(tf_test_dataset).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "y_true = np.concatenate([y for x, y in tf_test_dataset], axis=0)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.title('Confusion Matrix (Reduced Max Length)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Experiment 1B: Reduce Max Length Only (no class weights)\n",
    "\n",
    "# Tokenization (new max_length=128)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Train-test-validation split (same)\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Convert to TensorFlow datasets (same)\n",
    "tf_train_dataset = to_tf_dataset(train_val_split['train'], shuffle=True)\n",
    "tf_validation_dataset = to_tf_dataset(val_test_split['train'], shuffle=True)\n",
    "tf_test_dataset = to_tf_dataset(val_test_split['test'], shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Freeze encoder\n",
    "model.distilbert.trainable = False\n",
    "\n",
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train (no class weights)\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {eval_loss:.4f}, Test Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy (Reduced Max Length)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (Reduced Max Length)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix & Classification Report\n",
    "y_pred_logits = model.predict(tf_test_dataset).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "y_true = np.concatenate([y for x, y in tf_test_dataset], axis=0)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.title('Confusion Matrix (Reduced Max Length)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Experiment 1B: Reduce Max Length Only (no class weights)\n",
    "\n",
    "# Tokenization (new max_length=128)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Train-test-validation split (same)\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Convert to TensorFlow datasets (same)\n",
    "tf_train_dataset = to_tf_dataset(train_val_split['train'], shuffle=True)\n",
    "tf_validation_dataset = to_tf_dataset(val_test_split['train'], shuffle=True)\n",
    "tf_test_dataset = to_tf_dataset(val_test_split['test'], shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Freeze encoder\n",
    "model.distilbert.trainable = False\n",
    "\n",
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train (no class weights)\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {eval_loss:.4f}, Test Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy (Reduced Max Length)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (Reduced Max Length)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix & Classification Report\n",
    "y_pred_logits = model.predict(tf_test_dataset).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "y_true = np.concatenate([y for x, y in tf_test_dataset], axis=0)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.title('Confusion Matrix (Reduced Max Length)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
