{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCjxssoRcQIG"
      },
      "source": [
        "### 3.6 Working with TensorFlow Datasets\n",
        "\n",
        "In this section, we reproduce the baseline NLP setup using a `TensorFlow Dataset (TFDS)` pipeline rather than the Hugging Face `datasets` library. While Hugging Face datasets offer rich features and streamlined integration with Transformers, TFDS remains a robust alternative—often preferred in contexts where:\n",
        "\n",
        "- Native TensorFlow data pipelines are required,\n",
        "- Seamless integration with `tf.data` APIs and performance optimizations (e.g., prefetching, caching, parallelization) is desired,\n",
        "- Projects are bound to TensorFlow/TPU environments with minimal external dependencies.\n",
        "\n",
        "This notebook demonstrates compatibility with TFDS by mirroring one of the earlier experiments. However, this is a standalone detour meant purely for demonstration purposes—**no significant analytical insights were gained** here in the context of the project’s primary goals. For the remainder of the research, the Hugging Face `datasets` library will continue to be used for data handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install transformers scikit-learn pandas numpy tqdm tensorflow\n",
        "!pip install -q datasets\n",
        "\n",
        "# Imports\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NopeZM9Ic_p3",
        "outputId": "6ffd4bae-c656-43e3-bb2c-bb817182dd90"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Ensure reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "def load_raw_dataset():\n",
        "    return load_dataset(\"financial_phrasebank\", \"sentences_allagree\")[\"train\"]\n",
        "\n",
        "# Initialize tokenizer\n",
        "def load_tokenizer():\n",
        "    return AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", trust_remote_code=True)\n",
        "\n",
        "# Tokenize the dataset manually\n",
        "def tokenize_dataset(dataset_split, tokenizer, max_length=256):\n",
        "    input_ids, attention_masks, labels = [], [], []\n",
        "\n",
        "    for example in dataset_split:\n",
        "        encoded = tokenizer(\n",
        "            example[\"sentence\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length\n",
        "        )\n",
        "        input_ids.append(encoded[\"input_ids\"])\n",
        "        attention_masks.append(encoded[\"attention_mask\"])\n",
        "        labels.append(example[\"label\"])\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "# Build the full tf.data.Dataset and split AFTER creation\n",
        "def build_tf_datasets(input_ids, attention_masks, labels, batch_size=16, val_frac=0.1, test_frac=0.1):\n",
        "    total_size = len(input_ids)\n",
        "    val_size = int(val_frac * total_size)\n",
        "    test_size = int(test_frac * total_size)\n",
        "\n",
        "    full_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            \"input_ids\": tf.constant(input_ids),\n",
        "            \"attention_mask\": tf.constant(attention_masks)\n",
        "        },\n",
        "        tf.constant(labels)\n",
        "    ))\n",
        "\n",
        "    # Shuffle once and split into test, val, and train\n",
        "    full_dataset = full_dataset.shuffle(total_size, reshuffle_each_iteration=False)\n",
        "    test_ds = full_dataset.take(test_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    val_ds = full_dataset.skip(test_size).take(val_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    train_ds = full_dataset.skip(test_size + val_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "# Load and compile the model\n",
        "def load_model():\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\", num_labels=3\n",
        "    )\n",
        "    # Freeze encoder to reduce overfitting\n",
        "    model.distilbert.trainable = False\n",
        "\n",
        "    # Learning rate schedule\n",
        "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "        initial_learning_rate=5e-5,\n",
        "        decay_steps=1000,\n",
        "        end_learning_rate=1e-6,\n",
        "        power=1.0\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Main runner\n",
        "def main():\n",
        "    raw_dataset = load_raw_dataset()\n",
        "    tokenizer = load_tokenizer()\n",
        "    input_ids, attention_masks, labels = tokenize_dataset(raw_dataset, tokenizer)\n",
        "\n",
        "    train_ds, val_ds, test_ds = build_tf_datasets(input_ids, attention_masks, labels)\n",
        "\n",
        "    model = load_model()\n",
        "    model.summary()\n",
        "\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=2, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=3,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    eval_loss, eval_accuracy = model.evaluate(test_ds)\n",
        "    print(f\"\\nEvaluated Test Loss: {eval_loss:.4f}, Evaluated Test Accuracy: {eval_accuracy:.4f}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Run the pipeline\n",
        "model_manual_tfds, history_manual_tfds = main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qNb1LRaSeo8M",
        "outputId": "eb5cd0ef-beef-4cef-b9db-41c731d3160d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " distilbert (TFDistilBertMa  multiple                  66362880  \n",
            " inLayer)                                                        \n",
            "                                                                 \n",
            " pre_classifier (Dense)      multiple                  590592    \n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  2307      \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        multiple                  0 (unused)\n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66955779 (255.42 MB)\n",
            "Trainable params: 592899 (2.26 MB)\n",
            "Non-trainable params: 66362880 (253.15 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "114/114 [==============================] - 20s 84ms/step - loss: 0.8831 - accuracy: 0.6115 - val_loss: 0.7191 - val_accuracy: 0.6637\n",
            "Epoch 2/3\n",
            "114/114 [==============================] - 7s 65ms/step - loss: 0.7158 - accuracy: 0.6893 - val_loss: 0.5950 - val_accuracy: 0.7301\n",
            "Epoch 3/3\n",
            "114/114 [==============================] - 7s 66ms/step - loss: 0.6258 - accuracy: 0.7362 - val_loss: 0.5269 - val_accuracy: 0.7788\n",
            "15/15 [==============================] - 1s 46ms/step - loss: 0.5641 - accuracy: 0.7478\n",
            "\n",
            "Evaluated Test Loss: 0.5641, Evaluated Test Accuracy: 0.7478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why This Is Different\n",
        "\n",
        "In the main workflow, we typically:\n",
        "- Load the dataset using `datasets.load_dataset()` (Hugging Face)\n",
        "- Tokenize using `dataset.map(...)`\n",
        "- Convert directly to TensorFlow format using `.to_tf_dataset(...)`\n",
        "\n",
        "While convenient and efficient, this abstracts away the underlying mechanics of how TensorFlow ingests and prepares data for training. The `.to_tf_dataset()` method wraps a lot of logic—batching, formatting, and padding—into a single function.\n",
        "\n",
        "This demonstrates how to bridge between tokenized NLP data and TensorFlow training infrastructure without relying on helper abstractions.\n",
        "\n"
      ],
      "metadata": {
        "id": "cStbIPbPdEEe"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}