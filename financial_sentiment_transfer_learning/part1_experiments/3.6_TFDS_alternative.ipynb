{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: Working with TensorFlow Datasets\n",
    "\n",
    "The goal of this notebook is to show the same NLP setup except with an alteraction where we go from HF dataset to TFDS. This is sometimes preferred for x,y,z, but for the remainder of the research HF datasets will be used. \n",
    "\n",
    "This is purely to demonstrate the compatability with TFDS - no signficant analytical advances were made here in the context of the greater project goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset from Hugging Face and preprocess it\n",
    "def load_and_preprocess_data():\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    checkpoint = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"sentence\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    return tokenized_datasets\n",
    "\n",
    "# Create a TFDS for training, validation, and testing\n",
    "def create_tf_datasets(tokenized_datasets, batch_size=8):\n",
    "    # Convert Hugging Face datasets to TensorFlow datasets\n",
    "    tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=[\"label\"],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=None\n",
    "    )\n",
    "    return tf_train_dataset\n",
    "\n",
    "# Use the dataset for training a model\n",
    "def train_model(tf_train_dataset):\n",
    "    # Load the pre-trained model\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "    # Define the optimizer, loss, and metrics\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metrics = ['accuracy']\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    model_complete.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(tf_train_dataset, epochs=3)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Main function to run the steps\n",
    "def main():\n",
    "    # Load and preprocess the dataset\n",
    "    tokenized_datasets = load_and_preprocess_data()\n",
    "\n",
    "    # Create the TensorFlow datasets\n",
    "    tf_train_dataset = create_tf_datasets(tokenized_datasets)\n",
    "\n",
    "    # Train the model using the TensorFlow datasets\n",
    "    model_tfds, history_tfds = train_model(tf_train_dataset)\n",
    "\n",
    "    return model_tfds, history_tfds\n",
    "\n",
    "# Run the main function\n",
    "model_whole_tfds, history_whole_tfds = main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
