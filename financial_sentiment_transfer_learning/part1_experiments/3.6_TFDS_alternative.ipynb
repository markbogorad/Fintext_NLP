{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCjxssoRcQIG"
      },
      "source": [
        "### 3.6 Working with TensorFlow Datasets\n",
        "\n",
        "In this section, we reproduce the baseline NLP setup using a `TensorFlow Dataset (TFDS)` pipeline rather than the Hugging Face `datasets` library. While Hugging Face datasets offer rich features and streamlined integration with Transformers, TFDS remains a robust alternative—often preferred in contexts where:\n",
        "\n",
        "- Native TensorFlow data pipelines are required,\n",
        "- Seamless integration with `tf.data` APIs and performance optimizations (e.g., prefetching, caching, parallelization) is desired,\n",
        "- Projects are bound to TensorFlow/TPU environments with minimal external dependencies.\n",
        "\n",
        "This notebook demonstrates compatibility with TFDS by mirroring one of the earlier experiments. However, this is a standalone detour meant purely for demonstration purposes—**no significant analytical insights were gained** here in the context of the project’s primary goals. For the remainder of the research, the Hugging Face `datasets` library will continue to be used for data handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NopeZM9Ic_p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc_0a8BTcQII"
      },
      "outputs": [],
      "source": [
        "# Load dataset from Hugging Face\n",
        "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
        "checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Tokenize the dataset manually\n",
        "def encode_dataset(dataset_split, max_length=128):\n",
        "    input_ids, attention_masks, labels = [], [], []\n",
        "\n",
        "    for example in dataset_split:\n",
        "        encoded = tokenizer(\n",
        "            example[\"sentence\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length\n",
        "        )\n",
        "        input_ids.append(encoded[\"input_ids\"])\n",
        "        attention_masks.append(encoded[\"attention_mask\"])\n",
        "        labels.append(example[\"label\"])\n",
        "\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "# Encode training data only (demo)\n",
        "input_ids, attention_masks, labels = encode_dataset(dataset[\"train\"])\n",
        "\n",
        "# Manually build a tf.data.Dataset (NOT using .to_tf_dataset)\n",
        "def create_tf_dataset(input_ids, attention_masks, labels, batch_size=16):\n",
        "    tf_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            \"input_ids\": tf.constant(input_ids),\n",
        "            \"attention_mask\": tf.constant(attention_masks)\n",
        "        },\n",
        "        tf.constant(labels)\n",
        "    ))\n",
        "    return tf_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create TensorFlow dataset\n",
        "tf_dataset_manual = create_tf_dataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Load model and compile\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train\n",
        "history = model.fit(tf_dataset_manual, epochs=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why This Is Different\n",
        "\n",
        "In the main workflow, we typically:\n",
        "- Load the dataset using `datasets.load_dataset()` (Hugging Face)\n",
        "- Tokenize using `dataset.map(...)`\n",
        "- Convert directly to TensorFlow format using `.to_tf_dataset(...)`\n",
        "\n",
        "While convenient and efficient, this abstracts away the underlying mechanics of how TensorFlow ingests and prepares data for training. The `.to_tf_dataset()` method wraps a lot of logic—batching, formatting, and padding—into a single function.\n",
        "\n",
        "This demonstrates how to bridge between tokenized NLP data and TensorFlow training infrastructure without relying on helper abstractions.\n",
        "\n"
      ],
      "metadata": {
        "id": "cStbIPbPdEEe"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}