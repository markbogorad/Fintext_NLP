{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max_length sub testing\n",
    "\n",
    "### Results:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports (edit later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets scikit-learn pandas numpy tqdm tensorflow\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import load_dataset # Hugging Face\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Distilbert & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"all agree\" subset\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\") # All agree signifies 100% of annotators agreed on sentiment of this subset\n",
    "\n",
    "# Load tokenizer\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced Weights (Benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Train-test-validation split\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Convert to TensorFlow dataset\n",
    "def to_tf_dataset(split, shuffle=False):\n",
    "    return split.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=[\"label\"],\n",
    "        shuffle=shuffle,\n",
    "        batch_size=8,\n",
    "        collate_fn=None\n",
    "    )\n",
    "\n",
    "tf_train_dataset = to_tf_dataset(train_val_split['train'], shuffle=True)\n",
    "tf_validation_dataset = to_tf_dataset(val_test_split['train'], shuffle=True)\n",
    "tf_test_dataset = to_tf_dataset(val_test_split['test'], shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Freeze DistilBERT encoder\n",
    "model.distilbert.trainable = False\n",
    "\n",
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {eval_loss:.4f}, Test Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix & Classification Report\n",
    "y_pred_logits = model.predict(tf_test_dataset).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "\n",
    "y_true = np.concatenate([y for x, y in tf_test_dataset], axis=0)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Scaled Balanced Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment 1C: Apply Soft Class Weights Only\n",
    "\n",
    "# Tokenization (same as before)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Train-test-validation split (same)\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "tf_train_dataset = to_tf_dataset(train_val_split['train'], shuffle=True)\n",
    "tf_validation_dataset = to_tf_dataset(val_test_split['train'], shuffle=True)\n",
    "tf_test_dataset = to_tf_dataset(val_test_split['test'], shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Freeze encoder\n",
    "model.distilbert.trainable = False\n",
    "\n",
    "# Calculate softened class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "labels = np.array(train_val_split['train']['label'])\n",
    "balanced_class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "\n",
    "# Soft scaling factor (e.g., 0.5 to moderate the imbalance correction)\n",
    "scaling_factor = 0.5\n",
    "soft_class_weight_dict = {i: 1 + (balanced_class_weights[i] - 1) * scaling_factor for i in range(3)}\n",
    "\n",
    "print(\"Softened Class Weights:\", soft_class_weight_dict)\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train (using softened class weights)\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=3,\n",
    "    class_weight=soft_class_weight_dict\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {eval_loss:.4f}, Test Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy (Soft Class Weighted)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (Soft Class Weighted)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix & Classification Report\n",
    "y_pred_logits = model.predict(tf_test_dataset).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "y_true = np.concatenate([y for x, y in tf_test_dataset], axis=0)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.title('Confusion Matrix (Soft Class Weighted)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment 1D: Apply Custom Manual Class Weights\n",
    "\n",
    "# Tokenization (same as before)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Train-test-validation split (same)\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "tf_train_dataset = to_tf_dataset(train_val_split['train'], shuffle=True)\n",
    "tf_validation_dataset = to_tf_dataset(val_test_split['train'], shuffle=True)\n",
    "tf_test_dataset = to_tf_dataset(val_test_split['test'], shuffle=False)\n",
    "\n",
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Freeze encoder\n",
    "model.distilbert.trainable = False\n",
    "\n",
    "# Define custom class weights based on EDA\n",
    "custom_class_weight_dict = {\n",
    "    0: 1.8,  # Negative\n",
    "    1: 1.0,  # Neutral\n",
    "    2: 1.3   # Positive\n",
    "}\n",
    "\n",
    "print(\"Custom Class Weights:\", custom_class_weight_dict)\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=3,\n",
    "    class_weight=custom_class_weight_dict\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {eval_loss:.4f}, Test Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy (Custom Class Weights)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss (Custom Class Weights)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix & Classification Report\n",
    "y_pred_logits = model.predict(tf_test_dataset).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "y_true = np.concatenate([y for x, y in tf_test_dataset], axis=0)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.title('Confusion Matrix (Custom Class Weights)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
