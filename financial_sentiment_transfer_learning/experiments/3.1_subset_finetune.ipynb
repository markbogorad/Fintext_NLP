{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2epErt_-jGqj"
   },
   "source": [
    "# Subset Finetuning Experiments\n",
    "\n",
    "This experiment investigates how out-of-sample performance varies with the number of examples used during fine-tuning. Specifically, it aims to answer:\n",
    "\n",
    "- Does performance improve with more fine-tuning data?\n",
    "- What is the smallest subset size sufficient for generalization?\n",
    "- Does the specific subset chosen (among many possible) significantly affect results?\n",
    "\n",
    "All runs use the same training setup and hyperparameters. The training set size is varied across [16, 64, 128, 256, 512, 1024], and the results are compared to the full dataset baseline. Each subset size is evaluated using three random seeds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Up to 3.1 as Baseline"
   ],
   "metadata": {
    "id": "Pj4axU1hCh7R"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCQqmNayjGqk",
    "outputId": "e6768137-41b5-47e3-decd-0d4a5271461c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/491.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers datasets scikit-learn pandas numpy tqdm tensorflow\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFDistilBertModel,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "def to_tf_dataset(split, shuffle=False):\n",
    "    return split.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=[\"label\"],\n",
    "        shuffle=shuffle,\n",
    "        batch_size=16,\n",
    "        collate_fn=None\n",
    "    )\n",
    "\n",
    "tf_train_dataset = to_tf_dataset(train_val_split[\"train\"], shuffle=True)\n",
    "tf_validation_dataset = to_tf_dataset(val_test_split[\"train\"], shuffle=True)\n",
    "tf_test_dataset = to_tf_dataset(val_test_split[\"test\"], shuffle=False)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "model.distilbert.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_validation_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {eval_loss:.4f}, Test Accuracy: {eval_accuracy:.4f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680,
     "referenced_widgets": [
      "c9185f345b1d4b4ea2711304281f1a9c",
      "f2e8d4c68db241b28e794f4a30ec54da",
      "ec444569d597463ca11965db9777aebc",
      "6876e1fd409740dfa9cd27804e041a53",
      "b5f595c1055f45598bb79fe9cb80a119",
      "f0e2d2e96a5a4529a707e0de26d8831d",
      "5b5c0314f6a246c189f887a7717c40bb",
      "9a0c45cfb9544f1a83f00d924af834bf",
      "7f75439a3d124e59858b69fde279b4a1",
      "b14eb05b4a9747e2830f13b199880203",
      "086daa91ce174a6d917759afcf68c4af",
      "8e07307fc5e447799b44a2cfa75d1b99",
      "f5692abcb7b445038ea05d1000f25bd6",
      "f523bd6582d847979588460044e61e57",
      "62ab088878e4475696f29ffbcdff029a",
      "44651d15bb3142aab6359f99852cfdd6",
      "68d91ad763a544818100c7eebc31c7bf",
      "929148f49d0a4a3abc1458ebea2481e1",
      "507080147a894064983c7fe8c42dcfd2",
      "1692c75bc75e405989134d4e966cf8f0",
      "51a0877c234d4efaaff3fd4f1a12802e",
      "757ff12a9e5a439d9c61cf1bf6cb5e01",
      "958d50e508bd4c4fa536a36838360e38",
      "8e89a78486964a21bfb7a21675c8c788",
      "979d160a0a9d46dcae72163e9b942476",
      "bcf1962899334898b2155122423b71c4",
      "d92d15c0e3914a8592b0fb89b52152bf",
      "88a60e26b32444a89ddfd0b87aa1101f",
      "f2f69ea90006417b9dedcb8b9b74749a",
      "5e0fbbe76fef4638a1767df8bb670faa",
      "c0029495cb5b4efca999c1132f1d7516",
      "51cba6cb8d83430292bc32a140159065",
      "67a6954a5410485b98d663750008c5fa",
      "03f79bf9e648436899e5bfab75677b80",
      "bd79eac412494f12851942e8252bafbd",
      "c09944baf722493a9192f7333f1c0749",
      "0eaf7d25555744f189cc1b19676c704a",
      "9e9eaf8afaa74ce2a6741b65deff7ef2",
      "d396a7dc5a744bc1bf899fda118c0f77",
      "add944511f9545de82326ba6bfd65e91",
      "046b613bbb93476eb2332cdf72bee9ef",
      "f64fea6271b040d2b74f877fdadb603f",
      "ba12454171fe4667bf28c89aaa6ba761",
      "447fe2086d6a4534aa4bbe125632e07f",
      "eb0e11c11f1b4f508d6d0464de2803e3",
      "6750a8b1d9bb44f282787237ee5d762b",
      "0f3d0db3764d486da4250ef81e2140cb",
      "9451ece3ee8046f6a957fc78b4f87bdf",
      "7bb60a1cf87e4a1bb851b21ac1fbd7de",
      "c06f5e932dc8416c82bf0f96e5be2506",
      "5cc92737d6df4d3bb8e4ff818f828e6b",
      "738a383e38ce44a0bb1a87865c3a8485",
      "db597b4c17fa491b9b20b4e266d62430",
      "0b95e2fa648e41d9854969fa50288fb0",
      "45bbab3751e14f9f811d8388e9120afb",
      "964e22190a5a49d2abb139be378961ac",
      "4ea4dbb0f8cc439ba79035103b6dfee3",
      "4ce7b0e704b149d5ac29bd59a632e73e",
      "2c99d17ecce341e6a279a72ae989e302",
      "9e7b05ce3c2043f789370295e24fab80",
      "56586f73b94d48548385ea1b6d72bef1",
      "8a4af7e8c3d74aaea194f91b2b997e18",
      "6815d5f8c9ec4042a8ea62e3965a17ca",
      "a8ce05be400347fc978aa4916ebafb38",
      "486dc2b81ed249af879124db60c646c6",
      "e93a992d811d4e718f049ecd8a83a541",
      "9d86e6e25f094c42af02f6edb8a041fa",
      "ff548fc78a6942eb96495748335c1ae9",
      "4c9540477d2642ccb9bf90cffdbcfd35",
      "dd310d391179463b91a109826b3dfe04",
      "f6760b9fdfe54d5fbc7c86695e47f612",
      "ff116c116f2c4b7da24822163c75cd7b",
      "6290ec3df7544279b576721fefde57e4",
      "9fe465a25fea42cfb7f8c53629552799",
      "a7409e8119d346aaba8f9d59f5c33098",
      "fb74b236c3ad44c69835facb00d1c9ac",
      "ed6d984a4d5042089b9a7a9bc225c493",
      "13c9b03cb1d042468a10021efae755c3",
      "d68017519d754dd4b9138774d34eeeb6",
      "2957ec34c0bd4f69b52ddaaedc3f7ccc",
      "67e50eab59894f8aa192968984a77fad",
      "056aec8ce5234f2aa9c137c16850ab2d",
      "b49cf2eecadb481d978a1976431caab4",
      "add4463e8b4e47e18c2ea96b5cfb8447",
      "c7bf1aa8f9e347b6884592e00289081a",
      "a9e62f9b0a3b46a0a7cb73db174b2322",
      "8c7b2052905b422993eba7486dbd3759",
      "3860b62689194b51820ab214c3a340b5",
      "85ea7d31954d496187dad8ba7ba36906",
      "208a2b56c8db426b87e5dba21df03d70",
      "e99c7a4149024350820721cb2fa08c47",
      "41c6dd28c7744eeb88af334f2da427de",
      "e245ffba863049f2b372a21e07be3346",
      "6c1be127a87148d6b9345daefa49f6e6",
      "693bc9c84aa14f29809abe3ff7a0f63e",
      "b1493288188445b6939d5a1baec2e2dd",
      "44c74aa55dae42efa59c8b8b31e4800a",
      "2b917d59363f419398777f95b6c9b772",
      "3db3314c32a84a62a7ffad48d95d6df1",
      "ab491bb123f74ba6b751cdeeae14d33c",
      "ec29026907e74da6baba5954041bd442",
      "376f7b96c3a64efc9dad1148072bc402",
      "cb5ca880568d41c6bfe2458d75dccbf9",
      "ef560f7b1fc943db951d556443ad8e3e",
      "f0da3eec3a2b43fe912c64dec22b00d0",
      "e41b1dc9d11f40009e78c3310b3aa4a9",
      "a88a864a39ec4348891b0e90b8528875",
      "99ebc059146d4d9781f88dcb27e0ebe1",
      "74a1a1e25c9f4ea98c6ad8b460e4d8d7",
      "7f777355b0c2408593c02f08304403c0"
     ]
    },
    "id": "m5SjH0krC1qB",
    "outputId": "8ddae73c-3454-489d-a56f-c73a45c84651"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/8.88k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9185f345b1d4b4ea2711304281f1a9c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "financial_phrasebank.py:   0%|          | 0.00/6.04k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e07307fc5e447799b44a2cfa75d1b99"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for financial_phrasebank contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/financial_phrasebank.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N] y\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "FinancialPhraseBank-v1.0.zip:   0%|          | 0.00/682k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "958d50e508bd4c4fa536a36838360e38"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/2264 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03f79bf9e648436899e5bfab75677b80"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb0e11c11f1b4f508d6d0464de2803e3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "964e22190a5a49d2abb139be378961ac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d86e6e25f094c42af02f6edb8a041fa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13c9b03cb1d042468a10021efae755c3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2264 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85ea7d31954d496187dad8ba7ba36906"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab491bb123f74ba6b751cdeeae14d33c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "114/114 [==============================] - 20s 75ms/step - loss: 0.5554 - accuracy: 0.7543 - val_loss: 0.4255 - val_accuracy: 0.8097\n",
      "Epoch 2/3\n",
      "114/114 [==============================] - 7s 61ms/step - loss: 0.3974 - accuracy: 0.8222 - val_loss: 0.3875 - val_accuracy: 0.8186\n",
      "Epoch 3/3\n",
      "114/114 [==============================] - 7s 61ms/step - loss: 0.3750 - accuracy: 0.8299 - val_loss: 0.4293 - val_accuracy: 0.8319\n",
      "15/15 [==============================] - 1s 44ms/step - loss: 0.4118 - accuracy: 0.8502\n",
      "Test Loss: 0.4118, Test Accuracy: 0.8502\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subset Fine Tuning\n",
    "\n",
    "The subset sizes [16, 64, 128, 256, 512, 1024] were selected to cover a range from extremely limited data scenarios to near-complete usage of the available training set. This progression allows for the evaluation of both early performance gains and the point at which additional data yields diminishing returns.\n",
    "\n",
    "- 16: Represents an extreme low-resource setting.\n",
    "- 64: Tests for performance lift with modest data increase.\n",
    "- 128 and 256: Reflect typical sizes used in few-shot or low-resource fine-tuning contexts.\n",
    "- 512 and 1024: Used to assess performance stability and convergence toward full-data performance.\n",
    "\n",
    "The chosen sizes provide a practical and computationally efficient framework for analyzing the relationship between subset size and generalization."
   ],
   "metadata": {
    "id": "0cOZFYQIDSc7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "subset_sizes = [16, 64, 128, 256, 512, 1024]\n",
    "seeds = [0, 1, 2]\n",
    "\n",
    "results = []\n",
    "\n",
    "for size in subset_sizes:\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n\u25b6Training on subset size {size}, seed {seed}\")\n",
    "\n",
    "        # Fix seed for reproducibility\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Shuffle and sample subset from the train split\n",
    "        full_train = train_val_split[\"train\"].shuffle(seed=seed)\n",
    "        subset = full_train.select(range(min(size, len(full_train))))\n",
    "\n",
    "        # Create TF dataset\n",
    "        tf_subset_train_dataset = to_tf_dataset(subset, shuffle=True)\n",
    "\n",
    "        # Re-initialize the model each time to ensure fair training\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            checkpoint,\n",
    "            num_labels=3\n",
    "        )\n",
    "        model.distilbert.trainable = False\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "        # Train\n",
    "        model.fit(\n",
    "            tf_subset_train_dataset,\n",
    "            validation_data=tf_validation_dataset,\n",
    "            epochs=3,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # Evaluate\n",
    "        eval_loss, eval_accuracy = model.evaluate(tf_test_dataset, verbose=0)\n",
    "        print(f\"Subset {size}, Seed {seed}: Accuracy = {eval_accuracy:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"subset_size\": size,\n",
    "            \"seed\": seed,\n",
    "            \"test_accuracy\": eval_accuracy,\n",
    "            \"test_loss\": eval_loss\n",
    "        })"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiusGnixDRIs",
    "outputId": "4e4e2bad-e560-455d-be95-365deb6983a2"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u25b6Training on subset size 16, seed 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 16, Seed 0: Accuracy = 0.6256\n",
      "\n",
      "\u25b6Training on subset size 16, seed 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 16, Seed 1: Accuracy = 0.6256\n",
      "\n",
      "\u25b6Training on subset size 16, seed 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 16, Seed 2: Accuracy = 0.6256\n",
      "\n",
      "\u25b6Training on subset size 64, seed 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:tensorflow:5 out of the last 352 calls to <function Model.make_train_function.<locals>.train_function at 0x793aed6df9c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 64, Seed 0: Accuracy = 0.7621\n",
      "\n",
      "\u25b6Training on subset size 64, seed 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 64, Seed 1: Accuracy = 0.6608\n",
      "\n",
      "\u25b6Training on subset size 64, seed 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 64, Seed 2: Accuracy = 0.6696\n",
      "\n",
      "\u25b6Training on subset size 128, seed 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 128, Seed 0: Accuracy = 0.7709\n",
      "\n",
      "\u25b6Training on subset size 128, seed 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 128, Seed 1: Accuracy = 0.7753\n",
      "\n",
      "\u25b6Training on subset size 128, seed 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 128, Seed 2: Accuracy = 0.7621\n",
      "\n",
      "\u25b6Training on subset size 256, seed 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 256, Seed 0: Accuracy = 0.7974\n",
      "\n",
      "\u25b6Training on subset size 256, seed 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 256, Seed 1: Accuracy = 0.7930\n",
      "\n",
      "\u25b6Training on subset size 256, seed 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 256, Seed 2: Accuracy = 0.7841\n",
      "\n",
      "\u25b6Training on subset size 512, seed 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 512, Seed 0: Accuracy = 0.7753\n",
      "\n",
      "\u25b6Training on subset size 512, seed 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 512, Seed 1: Accuracy = 0.8150\n",
      "\n",
      "\u25b6Training on subset size 512, seed 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 512, Seed 2: Accuracy = 0.8150\n",
      "\n",
      "\u25b6Training on subset size 1024, seed 0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 1024, Seed 0: Accuracy = 0.8458\n",
      "\n",
      "\u25b6Training on subset size 1024, seed 1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 1024, Seed 1: Accuracy = 0.8062\n",
      "\n",
      "\u25b6Training on subset size 1024, seed 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Subset 1024, Seed 2: Accuracy = 0.7841\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"subset_size\", y=\"test_accuracy\", data=df)\n",
    "sns.stripplot(x=\"subset_size\", y=\"test_accuracy\", data=df, color='black', alpha=0.5)\n",
    "plt.title(\"Test Accuracy vs. Subset Size\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.xlabel(\"Subset Size\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "R425CuNwDZpG",
    "outputId": "6ad96a14-8f5a-4156-ead5-e31f5f2fc281"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "Test accuracy generally improves with larger subset sizes. Subsets of size 16 and 64 show high variability and lower average performance, indicating that they are insufficient for stable generalization. Accuracy stabilizes around size 128, and marginal improvements continue through 256 and beyond.\n",
    "\n",
    "Notably:\n",
    "- Performance becomes consistently strong by size 256.\n",
    "- Variance across random seeds narrows as the subset size increases.\n",
    "- Larger subsets (512, 1024) offer further gains but with diminishing returns.\n",
    "\n",
    "These results suggest that 128\u2013256 examples may be sufficient for reliable fine-tuning in this setting, while smaller subsets are highly sensitive to the specific examples selected.\n",
    "\n",
    "The findings imply that, for similar classification tasks, moderately sized labeled datasets may be adequate to achieve strong generalization, reducing the need for full-scale annotation. This has practical implications for domains where labeled data is limited or costly to obtain.\n",
    "\n",
    "## Research Questions\n",
    "\n",
    "- **Does performance improve with more fine-tuning data?**\n",
    " - Yes. Test accuracy improves consistently with increasing subset size. Larger subsets yield higher average accuracy and reduced variance across random seeds, indicating more stable generalization.\n",
    "\n",
    "- **What is the smallest subset size sufficient for generalization?**\n",
    "  - Subsets of size 128\u2013256 achieve performance within a close range of larger subsets, suggesting that this range may be sufficient for generalization in this classification task. Smaller subsets (16 and 64) show unstable and underperforming results.\n",
    "\n",
    "- **Does the specific subset chosen (among many possible) significantly affect results?**\n",
    "  - Yes, at smaller sizes. Variability in test accuracy across different seeds is substantial for subsets of size 64 and below, indicating that subset composition has a strong impact when data is limited. This effect diminishes as the subset size increases.\n"
   ],
   "metadata": {
    "id": "vWMFyyIlE7T9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Continuation back in [Main Notebook](../part1_finetuning_Distilbert.ipynb)"
   ],
   "metadata": {
    "id": "AK9CGAmvFXVL"
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}