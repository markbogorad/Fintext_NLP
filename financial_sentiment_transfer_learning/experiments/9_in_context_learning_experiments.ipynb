{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Context Learning\n",
    "\n",
    "In-context learning refers to the ability of a language model to perform a task by conditioning on a prompt that includes a small number of input-output examples, without any parameter updates. This experiment evaluates whether a pretrained language model can perform sentiment classification on financial text using in-context learning alone.\n",
    "\n",
    "The classification task involves assigning a label â€” positive, neutral, or negative â€” to financial news headlines. A subset of the Financial PhraseBank dataset is used, consisting of short, labeled financial sentences.\n",
    "\n",
    "The model used is Falcon-7B-Instruct, a publicly available large language model optimized for instruction-following tasks. It is accessed through Hugging Face and used in causal language modeling mode for prompt-based completion.\n",
    "\n",
    "For each test sentence, a prompt is constructed containing nine labeled examples (three per class). The model is asked to predict the sentiment of a new sentence based on these examples. To account for sampling variation, the experiment is repeated over five random seeds, and results are averaged.\n",
    "\n",
    "Across all runs, the model achieved an average accuracy of approximately 82% and a macro F1 score of approximately 73%. The confusion matrix from the final run showed improved coverage across all classes compared to earlier 5-shot experiments, indicating the effectiveness of balanced prompting for in-context learning in financial classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate bitsandbytes\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    f1_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Falcon-7B-Instruct\n",
    "*- Selected because it's open access*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",        # Automatically assigns GPU/CPU\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Finanical PhraseBank and Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "df = df.rename(columns={\"sentence\": \"text\", \"label\": \"true_label\"})\n",
    "\n",
    "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "df[\"label_text\"] = df[\"true_label\"].map(label_map)\n",
    "\n",
    "# Shuffle and sample\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "few_shot = df[:5]\n",
    "test_set = df[5:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Builder\n",
    "\n",
    "This function constructs a prompt for few-shot in-context learning. It formats a series of labeled financial headlines followed by an unlabeled test sentence. Each example follows the pattern: `\"headline\" â†’ Label`. The final sentence ends with an arrow, prompting the model to complete the sentiment label based on the preceding examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(few_shot_df, test_sentence):\n",
    "    prompt = \"Classify the sentiment of each financial headline as Positive, Neutral, or Negative.\\n\\n\"\n",
    "    for i, row in few_shot_df.iterrows():\n",
    "        prompt += f\"{i+1}. \\\"{row['text']}\\\" â†’ {row['label_text'].capitalize()}\\n\"\n",
    "    prompt += f\"{len(few_shot_df)+1}. \\\"{test_sentence}\\\" â†’\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Loop\n",
    "\n",
    "This loop generates a prediction for each test sentence by inserting it into a prompt with few-shot examples, passing the prompt to the model, and extracting the predicted sentiment from the modelâ€™s response. The output is matched to one of the expected labels or marked as \"unknown\" if no match is found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_tokens=20):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded[len(prompt):].strip().lower()\n",
    "\n",
    "predictions = []\n",
    "for i, row in test_set.iterrows():\n",
    "    prompt = build_prompt(few_shot, row[\"text\"])\n",
    "    response = generate_response(prompt)\n",
    "    for label in [\"positive\", \"neutral\", \"negative\"]:\n",
    "        if label in response:\n",
    "            predictions.append(label)\n",
    "            break\n",
    "    else:\n",
    "        predictions.append(\"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "cm = confusion_matrix(true, pred, labels=labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp.plot(cmap=\"Blues\", ax=ax, colorbar=True)\n",
    "plt.title(\"ðŸ§  Falcon In-Context Learning Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The initial evaluation using a single 9-shot prompt resulted in an accuracy of 80%. The model correctly identified all neutral and negative examples but failed to classify any positive examples correctly. Most predictions defaulted to the neutral class, indicating a bias toward the majority class in the few-shot prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shot Prompting\n",
    "\n",
    "This section refines the initial experiment by repeating the 9-shot in-context learning setup over multiple random seeds. For each run, a new set of few-shot examples is sampled to reduce bias from any specific prompt configuration. The model generates predictions for the same test set in each run, and the results are averaged. This approach helps account for variability in prompt composition and provides a more stable estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42, 99, 123, 2025, 7]\n",
    "accuracies = []\n",
    "f1_macros = []\n",
    "\n",
    "for seed in seeds:\n",
    "    few_shot = sample_few_shot_balanced(df, shots_per_class=3, seed=seed)\n",
    "\n",
    "    preds = []\n",
    "    for _, row in test_set.iterrows():\n",
    "        prompt = build_prompt(few_shot, row[\"text\"])\n",
    "        response = generate_response(prompt)\n",
    "        for label in [\"positive\", \"neutral\", \"negative\"]:\n",
    "            if label in response:\n",
    "                preds.append(label)\n",
    "                break\n",
    "        else:\n",
    "            preds.append(\"unknown\")\n",
    "\n",
    "    # Save prediction to test set for the *last* seed\n",
    "    if seed == seeds[-1]:\n",
    "        test_set[\"predicted_label_9shot\"] = preds\n",
    "\n",
    "    true = test_set[\"label_text\"]\n",
    "    pred = preds\n",
    "    acc = accuracy_score(true, pred)\n",
    "    f1 = f1_score(true, pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    f1_macros.append(f1)\n",
    "\n",
    "# Display average performance\n",
    "print(f\"Averaged over {len(seeds)} seeds:\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.2f}\")\n",
    "print(f\"Mean Macro F1 Score: {np.mean(f1_macros):.2f}\")\n",
    "\n",
    "# Plot confusion matrix from last run\n",
    "labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "cm = confusion_matrix(test_set[\"label_text\"], test_set[\"predicted_label_9shot\"], labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ConfusionMatrixDisplay(cm, display_labels=labels).plot(cmap=\"Blues\", ax=ax)\n",
    "plt.title(\"Falcon In-Context Learning Confusion Matrix (9-shot Prompting, Last Seed)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shot Prompting Results\n",
    "\n",
    "Averaging across five runs with different few-shot samples produced more stable performance. The model achieved a mean accuracy of approximately 82% and a macro F1 score of around 73%. Compared to the initial single-run result, this approach reduced variance and improved coverage across all sentiment classes, particularly for the underrepresented positive class. The final confusion matrix showed balanced predictions and confirmed that prompt diversity contributes to more reliable in-context learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuation back in [Main Notebook](../part1_finetuning_Distilbert.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
