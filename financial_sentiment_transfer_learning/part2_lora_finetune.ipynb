{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='text-align:center; font-size:30px; font-weight:bold; '>LoRA Fine-Tuning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Part 2 Introduction\n",
    "\n",
    "2. LoRA-Based Fine-Tuning\n",
    "   - 2.1 Train Classifier Head Only \n",
    "   - 2.2 Fine Tune All Weights with Classifier Head Carried Over  \n",
    "   - 2.3 LoRA Technique – Fine Tuning Adapters (FNN Blocks)\n",
    "   - 2.4 Experiment: Varying Adapters\n",
    "\n",
    "3. Conclusion and Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers scikit-learn pandas numpy tqdm tensorflow\n",
    "!pip install -q datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import load_dataset # Hugging Face\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Train Classifier Head Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_val_split = tokenized_datasets[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "val_test_split = train_val_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "def to_tf_dataset(split, shuffle=False):\n",
    "    return split.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=[\"label\"],\n",
    "        shuffle=shuffle,\n",
    "        batch_size=8,\n",
    "        collate_fn=None\n",
    "    )\n",
    "\n",
    "tf_train_dataset = to_tf_dataset(train_val_split['train'], shuffle=True)\n",
    "tf_validation_dataset = to_tf_dataset(val_test_split['train'], shuffle=True)\n",
    "tf_test_dataset = to_tf_dataset(val_test_split['test'], shuffle=False)\n",
    "\n",
    "# Define and compile model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "model.distilbert.trainable = False\n",
    "\n",
    "initial_learning_rate = 5e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    end_learning_rate=0.0,\n",
    "    power=1.0\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Training\n",
    "model.fit(tf_train_dataset, epochs=3, validation_data=tf_validation_dataset)\n",
    "\n",
    "# Evaluation\n",
    "eval_loss, eval_accuracy = model.evaluate(tf_test_dataset)\n",
    "print(f\"Evaluated Test Loss: {eval_loss}, Evaluated Test Accuracy: {eval_accuracy}\")\n",
    "\n",
    "# Save weights\n",
    "model.save_weights('classifier_head_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Fine Tune All Weights with Classifier Head Carried Over  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model for fine-tuning all layers except the classifier\n",
    "model_minus_classifier = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "# Now load the weights saved back into the model\n",
    "model_minus_classifier.load_weights('classifier_head_weights.h5', by_name=True)\n",
    "\n",
    "# Set only the pre_classifier and classifier layer to non-trainable to preserve its weights\n",
    "model_minus_classifier.pre_classifier.trainable = False\n",
    "model_minus_classifier.classifier.trainable = False\n",
    "\n",
    "initial_learning_rate = 5e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    end_learning_rate=0.0,\n",
    "    power=1.0\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model_minus_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_minus_classifier.summary()\n",
    "\n",
    "history = model_minus_classifier.fit(\n",
    "    tf_train_dataset,\n",
    "    epochs=3,\n",
    "    validation_data=tf_validation_dataset\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_minus_classifier.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 LoRA Technique – Fine Tuning Adapters (FNN Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "\n",
    "class LoRALayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, original_layer, rank):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.input_dim = original_layer.kernel.shape[0]\n",
    "        self.output_dim = original_layer.kernel.shape[1]\n",
    "\n",
    "        # Initialize LoRA parameters\n",
    "        self.A = self.add_weight(shape=(self.input_dim, rank), initializer='glorot_uniform', trainable=True)\n",
    "        self.B = self.add_weight(shape=(rank, self.output_dim), initializer='glorot_uniform', trainable=True)\n",
    "        self.lora_alpha = 1.0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Compute the original output\n",
    "        original_output = self.original_layer(inputs)\n",
    "\n",
    "        # Compute the low-rank adaptation\n",
    "        lora_update = self.lora_alpha * tf.matmul(inputs, tf.matmul(self.A, self.B))\n",
    "\n",
    "        return original_output + lora_update\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "model_lora = TFDistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "# Load the previously saved weights into the model\n",
    "model_lora.load_weights('classifier_head_weights.h5', by_name=True)\n",
    "\n",
    "# Rank for LoRA matrices\n",
    "lora_rank = 8\n",
    "\n",
    "# Replace each lin1 and lin2 layer in each transformer layer with a LoRA-enhanced layer\n",
    "for i, layer in enumerate(model_lora.distilbert.transformer.layer):\n",
    "    original_lin1 = layer.ffn.lin1\n",
    "    original_lin2 = layer.ffn.lin2\n",
    "    layer.ffn.lin1 = LoRALayer(original_lin1, rank=lora_rank)\n",
    "    layer.ffn.lin2 = LoRALayer(original_lin2, rank=lora_rank)\n",
    "\n",
    "    original_lin1.trainable = False\n",
    "    original_lin2.trainable = False\n",
    "\n",
    "# Freeze weights of the classifier head\n",
    "model_lora.pre_classifier.trainable = False\n",
    "model_lora.classifier.trainable = False\n",
    "\n",
    "# Freeze all other parameters except for the LoRA parameters\n",
    "for layer in model_lora.distilbert.transformer.layer:\n",
    "    layer.attention.trainable = False\n",
    "\n",
    "initial_learning_rate = 5e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    end_learning_rate=0.0,\n",
    "    power=1.0\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model_lora.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "model_lora.summary()\n",
    "\n",
    "# Train the model\n",
    "model_lora.fit(tf_train_dataset, epochs=3, validation_data=tf_validation_dataset)\n",
    "\n",
    "test_loss, test_accuracy = model_lora.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Experiment: Varying Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "\n",
    "class LoRALayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, original_layer, rank):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.input_dim = original_layer.kernel.shape[0]\n",
    "        self.output_dim = original_layer.kernel.shape[1]\n",
    "\n",
    "        # Initialize LoRA parameters\n",
    "        self.A = self.add_weight(shape=(self.input_dim, rank), initializer='glorot_uniform', trainable=True)\n",
    "        self.B = self.add_weight(shape=(rank, self.output_dim), initializer='glorot_uniform', trainable=True)\n",
    "        self.lora_alpha = 1.0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        original_output = self.original_layer(inputs)\n",
    "        lora_update = self.lora_alpha * tf.matmul(inputs, tf.matmul(self.A, self.B))\n",
    "        return original_output + lora_update\n",
    "\n",
    "# List of ranks to test\n",
    "ranks = [2, 4, 8, 16, 32, 64]\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for lora_rank in ranks:\n",
    "    print(f\"Training with LoRA rank: {lora_rank}\")\n",
    "    model_lora = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "    model_lora.load_weights('classifier_head_weights.h5', by_name=True)\n",
    "\n",
    "    # Apply LoRA layers\n",
    "    for i, layer in enumerate(model_lora.distilbert.transformer.layer):\n",
    "        original_lin1 = layer.ffn.lin1\n",
    "        original_lin2 = layer.ffn.lin2\n",
    "        layer.ffn.lin1 = LoRALayer(original_lin1, rank=lora_rank)\n",
    "        layer.ffn.lin2 = LoRALayer(original_lin2, rank=lora_rank)\n",
    "\n",
    "        original_lin1.trainable = False\n",
    "        original_lin2.trainable = False\n",
    "\n",
    "    # Freeze classifier head\n",
    "    model_lora.pre_classifier.trainable = False\n",
    "    model_lora.classifier.trainable = False\n",
    "\n",
    "    # Freeze all other parameters except for the LoRA parameters\n",
    "    for layer in model_lora.distilbert.transformer.layer:\n",
    "        layer.attention.trainable = False\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        initial_learning_rate=5e-5,\n",
    "        decay_steps=10000,\n",
    "        end_learning_rate=0.0,\n",
    "        power=1.0\n",
    "    ))\n",
    "    model_lora.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "    model_lora.summary()\n",
    "\n",
    "    # Train the model\n",
    "    model_lora.fit(tf_train_dataset, epochs=3, validation_data=tf_validation_dataset)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = model_lora.evaluate(tf_test_dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(f\"Rank: {lora_rank}, Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ranks, test_losses, marker='o', color='b')\n",
    "plt.title('Rank vs Test Loss')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Test Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ranks, test_accuracies, marker='o', color='r')\n",
    "plt.title('Rank vs Test Accuracy')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Test Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other LoRA Features\n",
    "- LoRA vs Pre-Trained Model\n",
    "- LoRA vs conventional fine tuning\n",
    "- LoRA Parameters per Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model for fine-tuning all layers except the classifier\n",
    "model_without_pre_training = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "\n",
    "initial_learning_rate = 5e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    end_learning_rate=0.0,\n",
    "    power=1.0\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model_without_pre_training.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_without_pre_training.summary()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model_without_pre_training.evaluate(tf_test_dataset)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "# Params\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "# Retrieve the input and output dimensions from the first FFN block\n",
    "d_input = model.distilbert.transformer.layer[0].ffn.lin1.kernel.shape[0]\n",
    "d_output = model.distilbert.transformer.layer[0].ffn.lin1.kernel.shape[1]\n",
    "\n",
    "print(f\"Input dimension (d_input): {d_input}\")\n",
    "print(f\"Output dimension (d_output): {d_output}\")\n",
    "\n",
    "r = 8  # Rank used for LoRA\n",
    "\n",
    "# Calculate number of parameters for one FFN block (lin1 and lin2)\n",
    "def calculate_lora_parameters(d_input, d_output, rank):\n",
    "    return (d_input * rank) + (rank * d_output)\n",
    "\n",
    "lora_params_per_layer = calculate_lora_parameters(d_input, d_output, r)\n",
    "\n",
    "lora_params_per_ffn_block = lora_params_per_layer * 2\n",
    "\n",
    "print(f\"Number of LoRA parameters per layer: {lora_params_per_layer}\")\n",
    "print(f\"Total LoRA parameters per FFN block: {lora_params_per_ffn_block}\")\n",
    "\n",
    "# Total params\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "num_layers = len(model.distilbert.transformer.layer)\n",
    "\n",
    "print(f\"Number of transformer layers: {num_layers}\")\n",
    "added_total = lora_params_per_ffn_block * num_layers\n",
    "\n",
    "print(f\"Number of LoRA parameters per layer: {lora_params_per_layer}\")\n",
    "print(f\"Total LoRA parameters per FFN block: {lora_params_per_ffn_block}\")\n",
    "print(f\"Total added LoRA parameters across all FFN blocks: {added_total}\")\n",
    "\n",
    "# Total parameters in the MODEL\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(np.prod(v.shape) for v in model.trainable_weights)\n",
    "\n",
    "# Calculate the total number of parameters in the unmodified DistilBERT model\n",
    "original_params = count_parameters(model)\n",
    "\n",
    "print(f\"Original DistilBERT parameters: {original_params}\")\n",
    "# Total parameters in the model\n",
    "total_parameters = original_params + added_total\n",
    "\n",
    "print(f\"Original DistilBERT parameters: {original_params}\")\n",
    "print(f\"Added LoRA parameters: {added_total}\")\n",
    "print(f\"Total parameters in the adapted model: {total_parameters}\")\n",
    "\n",
    "# trainable LoRA params\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "d_input = model.distilbert.transformer.layer[0].ffn.lin1.kernel.shape[0]\n",
    "d_output = model.distilbert.transformer.layer[0].ffn.lin1.kernel.shape[1]\n",
    "num_layers = len(model.distilbert.transformer.layer)\n",
    "r = 8  # Rank used for LoRA\n",
    "\n",
    "def calculate_lora_parameters(d_input, d_output, rank):\n",
    "    return (d_input * rank) + (rank * d_output)\n",
    "\n",
    "lora_params_per_layer = calculate_lora_parameters(d_input, d_output, r)\n",
    "\n",
    "lora_params_per_ffn_block = lora_params_per_layer * 2\n",
    "\n",
    "added_parms_calc = lora_params_per_ffn_block * num_layers\n",
    "\n",
    "added_total = added_parms_calc\n",
    "\n",
    "# Assertion to verify the numbers match\n",
    "assert(added_total == added_parms_calc)\n",
    "\n",
    "print(f\"Calculated trainable parameters added by LoRA: {added_parms_calc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion and Key Takeaways"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
